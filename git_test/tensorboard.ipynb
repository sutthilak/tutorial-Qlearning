{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e972c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback,StopTrainingOnRewardThreshold,EvalCallback\n",
    "log_path = os.path.join('Training','Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8c2b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kwan\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n",
      "C:\\Users\\Kwan\\AppData\\Roaming\\Python\\Python38\\site-packages\\stable_baselines3\\common\\env_checker.py:272: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.ddpg.ddpg.DDPG'>\n",
      "[]\n",
      "1741303.0 0.0\n",
      "[200. 400. 100. 200.]\n",
      "[200. 400. 100. 200.]\n",
      "[200. 400. 100. 200.]\n",
      "[200. 400. 100. 200.]\n",
      "[200. 400. 100. 200.]\n",
      "[200. 400. 100. 200.]\n",
      "[200. 400. 100. 200.]\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kwan\\AppData\\Roaming\\Python\\Python38\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from gym.core import RewardWrapper\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "from gym.spaces import MultiDiscrete, Box\n",
    "import numpy as np\n",
    "from numpy.core.fromnumeric import mean\n",
    "from numpy.core.numeric import True_\n",
    "from numpy.lib.function_base import average\n",
    "from stable_baselines3 import DDPG,A2C,SAC,TD3,PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.noise import ActionNoise\n",
    "#from torch._C import DoubleTensor\n",
    "#from torch._C import TreeView\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SimpleEnv(gym.Env):\n",
    "\n",
    "    \"\"\"\n",
    "     Description:\n",
    "         The agent (control signal to G number of generators) is given a scalar value D at each timestep.\n",
    "         At each timestep the agent can choose how much energy each generator shall generate, with the goal\n",
    "         to match the demand at the given timestep.\n",
    "     Observation:\n",
    "         Type: Box(1)\n",
    "         Num    Observation               Min            Max\n",
    "         0      Demand                    0          0.07\n",
    "     Actions:\n",
    "         Type: Box(G)\n",
    "         Num    Action                    Min            Max\n",
    "         0      Generation of gen 1..     p_low          p_high\n",
    "         1      ...\n",
    "         .\n",
    "         .\n",
    "         G      ...\n",
    "     Reward:\n",
    "         Reward is calculated as the sum of generation costs of all generators and the difference\n",
    "         in power generation and demand.\n",
    "         Generation costs are calculated according to the quadratic cost function:\n",
    "         C_gen = a*p^2 + b*p + c for each generator, and difference is calculated as:\n",
    "         C_diff = k * abs( sum(p) - D ) where k is a coefficient. (more elaborate cost can be implemented later)\n",
    "         The reward is then equal to the negative cost:\n",
    "         R = -(C_gen + C_diff) at each timestep\n",
    "     Starting State:\n",
    "         The state evolution follows the given time-series of demands D.\n",
    "     Episode Termination:\n",
    "         When the end of the time series of demands is reached, the episode terminates.\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, gen_args=[[200, 600, 0.002, 10, 500, 50]], D=[800, 850, 880, 900, 860, 930, 950]):\n",
    "        self.D = D\n",
    "        self.gen_list = []\n",
    "        for args in gen_args:\n",
    "            self.gen_list.append(Generator(*args))\n",
    "        self.action_space = Box(low=np.array([gen.p_low for gen in self.gen_list]), high=np.array(\n",
    "            [gen.p_high for gen in self.gen_list]))\n",
    "        self.observation_space = Box(\n",
    "            low=np.array([0]), high=np.array([1000]),)\n",
    "        self.state_length = len(self.D)\n",
    "        self.index = 0\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Input: action (tuple)\n",
    "        \"\"\"\n",
    "        gen_reward = 0\n",
    "        # Sum all actions (generation of each generator) and compare to demand\n",
    "        total_p = 0\n",
    "        for i, p in enumerate(action):\n",
    "            total_p += p\n",
    "            # Extract parameters of current generator\n",
    "            gen = self.gen_list[i]\n",
    "            # Also add costs of generation to rewards\n",
    "            gen_reward -= gen.a*p**2 + gen.b*p + gen.c\n",
    "\n",
    "        # Cost constants\n",
    "        k1 = 1 # 0.001  # generation \n",
    "        k2 = 1  # difference\n",
    "        k21 = 1  # overproduction\n",
    "        k22 = 1  # underproduction\n",
    "        diff = total_p-self.D[self.index]\n",
    "        if diff >= 0:\n",
    "            diff_reward = k21*diff\n",
    "        else: \n",
    "            diff_reward = -k22*diff\n",
    "        reward = -k1*gen_reward - k2*diff_reward\n",
    "\n",
    "        if self.index == self.state_length-1:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "            # Increment index\n",
    "            self.index += 1\n",
    "            self.state = np.array([self.D[self.index]], dtype=np.float32)\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        self.state = np.array([self.D[self.index]], dtype=np.float32)\n",
    "        return self.state\n",
    "\n",
    "    # Not needed\n",
    "    # def render(self, mode='human'):\n",
    "        ...\n",
    "    # def close(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self, p_low, p_high, a, c, b, p_r):\n",
    "        self.p_low = p_low\n",
    "        self.p_high = p_high\n",
    "        self.a = a\n",
    "        self.c = c\n",
    "        self.b = b\n",
    "        self.p_r = p_r\n",
    "\n",
    "def main():\n",
    "\n",
    "    arg1 = [200, 600, 0.002, 10, 500, 50]\n",
    "    arg2 = [100, 400, 0.0025, 8, 300, 50]\n",
    "    arg3 = [100, 300, 0.0050, 6, 100, 50]\n",
    "    arg4 = [50, 200, 0.0060,  5, 90, 50]\n",
    "    gen_args = [arg1, arg2, arg3, arg4]\n",
    "    env = SimpleEnv(gen_args)\n",
    "    \n",
    "    MODELS = [DDPG] #,A2C,TD3,PPO,SAC\n",
    "    total_timesteps=20000\n",
    "    model_rewards = []\n",
    "    \n",
    "\n",
    "\n",
    "    for m in MODELS:\n",
    "        print(m)\n",
    "        callback =CustomCallback(model=m)\n",
    "        model= m('MlpPolicy', env, verbose=0,tensorboard_log=log_path).learn(total_timesteps=total_timesteps,callback=callback)\n",
    "        print(callback.episode_rewards)\n",
    "        done = False \n",
    "        obs = env.reset()\n",
    "        current_reward = 0\n",
    "        try:\n",
    "            mean_reward , standard_deviation = evaluate_policy(model, env ,n_eval_episodes=10,render=False)\n",
    "            print (mean_reward, standard_deviation)\n",
    "        except:\n",
    "            pass\n",
    "        while not done:\n",
    "            action,_states = model.predict(obs)\n",
    "            obs,reward,done,info =env.step(action)\n",
    "            print(action) \n",
    "            current_reward += reward\n",
    "            \n",
    "        \n",
    "        model_rewards.append(current_reward)\n",
    "        obs = env.reset()  \n",
    "        del model\n",
    "        \n",
    "        print('------------------------------')\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class CustomCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "    def __init__(self,model, verbose=0):\n",
    "        self.model= model\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.episode_rewards =[]\n",
    "        self.episode_count = 0\n",
    "        self.episode_lenght = 0\n",
    "        self.current_rewards = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: (bool) If the callback returns False, training is aborted early.\n",
    "        \"\"\" \n",
    "        if self.model is A2C:\n",
    "            self.current_rewards += self.locals['rewards']\n",
    "            if self.locals['done']:\n",
    "                self.episode_rewards.append(self.current_rewards)\n",
    "                self.episode_lenght += 1 \n",
    "\n",
    "        #else :\n",
    "            #f self.locals['done']:\n",
    "                #self.episode_rewards.append(self.locals['episode_rewards'])\n",
    "                #self.episode_lenght += 1 \n",
    "\n",
    "        return True\n",
    "        \n",
    "    def _on_rollout_end(self):\n",
    "        # Not needed \n",
    "        pass\n",
    "    def _on_training_end(self):\n",
    "        # for debugging \n",
    "        '''\n",
    "        print(self.locals)\n",
    "        print(self.globals)\n",
    "        print(self.model)\n",
    "        try:\n",
    "            print(self.locals['done'])\n",
    "        except:\n",
    "            print(self.locals['dones'])\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n",
    "    main()\n",
    "\n",
    "# env = SimpleEnv()\n",
    "# action = env.action_space.sample()\n",
    "# print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da3dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path =os.path.join(log_path,'DDPG_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f443a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ced1f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir={training_log_path}\n",
    "# http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab30eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path =os.path.join('Training','SavedModels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
    "eval_callback = EvalCallback(env, callback_on_new_best = stop_callback,\n",
    "                            eval_freq=10000,\n",
    "                            best_model_save_path=save_path,\n",
    "                            verbose =1 )\n",
    "\n",
    "model =PPO('MlpPolicy',env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=20000,callback=eval_callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
